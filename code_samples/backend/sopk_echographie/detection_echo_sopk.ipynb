{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "544y28G8lXq5",
        "outputId": "5c9720b3-2bfd-4ead-b3d6-0d324138e8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50 , EfficientNetB0 , MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "hf87uxYblsXL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparer les données avec l'augmentation\n",
        "On utilise ImageDataGenerator pour prétraiter et augmenter les images"
      ],
      "metadata": {
        "id": "7oezI_CCoZm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Répertoires des données\n",
        "train_dir = '/content/drive/MyDrive/dataset_echo_sopk/train'\n",
        "val_dir  = '/content/drive/MyDrive/dataset_echo_sopk/test'\n",
        "\n",
        "img_size = (224, 224)\n",
        "batch_size = 16  #32\n",
        "\n",
        "\n",
        "# Générateurs de données avec augmentation pour l'entraînement\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # rescale=1. / 255,\n",
        "    # rotation_range=45,\n",
        "    # width_shift_range=0.3,\n",
        "    # height_shift_range=0.3,\n",
        "    # shear_range=0.5,\n",
        "    # zoom_range=0.5,\n",
        "    # horizontal_flip=True,\n",
        "    # vertical_flip=True,\n",
        "    # fill_mode='nearest'\n",
        "     rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "# Générateur de validation (pas d'augmentation ici)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Charger les images\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "print(train_generator.class_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRtAS_tQoZSJ",
        "outputId": "5a47be0d-e3c1-4308-81a5-77d6bf0eba75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2928 images belonging to 2 classes.\n",
            "Found 300 images belonging to 2 classes.\n",
            "{'infected': 0, 'notinfected': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Définition des callbacks pour éviter l'overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n"
      ],
      "metadata": {
        "id": "l3YRLaKSpvQ1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "pYkSratRp0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Construire le modèle CNN\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # pour la classification binaire\n",
        "])\n"
      ],
      "metadata": {
        "id": "NMf3pn6pp0SP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler et Entraîner le modèle\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data = val_generator ,\n",
        "    epochs=10,  # Modifier selon les besoins\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2t2gDrwqH6q",
        "outputId": "2961a78e-b101-405d-8e4b-c619d0cbb8eb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 265ms/step - accuracy: 0.9944 - loss: 0.0335 - val_accuracy: 1.0000 - val_loss: 3.1802e-04 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 255ms/step - accuracy: 0.9946 - loss: 0.0139 - val_accuracy: 1.0000 - val_loss: 7.3682e-05 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 250ms/step - accuracy: 0.9982 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.1307e-05 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 257ms/step - accuracy: 1.0000 - loss: 2.0623e-04 - val_accuracy: 1.0000 - val_loss: 5.2982e-06 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 251ms/step - accuracy: 0.9999 - loss: 5.5425e-04 - val_accuracy: 1.0000 - val_loss: 1.2038e-05 - learning_rate: 2.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 262ms/step - accuracy: 1.0000 - loss: 8.8289e-04 - val_accuracy: 1.0000 - val_loss: 1.2130e-06 - learning_rate: 2.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 256ms/step - accuracy: 0.9993 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 1.4742e-06 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 256ms/step - accuracy: 1.0000 - loss: 3.1321e-05 - val_accuracy: 1.0000 - val_loss: 1.1259e-06 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 250ms/step - accuracy: 1.0000 - loss: 1.0167e-04 - val_accuracy: 1.0000 - val_loss: 1.1113e-06 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 252ms/step - accuracy: 1.0000 - loss: 2.2993e-05 - val_accuracy: 1.0000 - val_loss: 6.3254e-07 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset_echo_sopk/test',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrWN46FgqO2Z",
        "outputId": "13a7c08c-d329-42f3-8a35-cd522c3d32d0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 3.0828e-06\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Chemin de l'image à tester\n",
        "img_path = \"/content/drive/MyDrive/dataset_echo_sopk/test_final/notinfected1.jpg\"\n",
        "\n",
        "# Charger et préparer l'image\n",
        "image = cv2.imread(img_path)                         # Lire l'image\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       # Convertir BGR -> RGB\n",
        "image = cv2.resize(image, (224, 224))                # Redimensionner à la taille du modèle\n",
        "image = image / 255.0                                # Normaliser les pixels entre 0 et 1\n",
        "image = np.expand_dims(image, axis=0)                # Ajouter la dimension batch (1, 224, 224, 3)\n",
        "\n",
        "# Prédiction\n",
        "prediction = model.predict(image)[0][0]\n",
        "\n",
        "# Résultat\n",
        "if prediction < 0.5:\n",
        "    print(f\"L'ovaire est infecté (SOPK détecté) avec {(1 - prediction) * 100:.2f}% de certitude\")\n",
        "else:\n",
        "    print(f\"L'ovaire est sain avec {prediction * 100:.2f}% de certitude\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyFCW0Wi8j-B",
        "outputId": "ab8119ec-2eb5-4711-a2f1-110cacae96b5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ce99c2abe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
            "L'ovaire est sain avec 100.00% de certitude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50"
      ],
      "metadata": {
        "id": "BFNJz49b_Qjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger ResNet50 + ajouter des couches finales\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Geler les couches du modèle pré-entraîné\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Ajouter des couches finales\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # binaire : malade / non\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n"
      ],
      "metadata": {
        "id": "T8G28spX_HQO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler et entraîner\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    # callbacks=[early_stop]\n",
        "     callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8yKALUR_zny",
        "outputId": "c88966b1-1450-4967-f0dc-9697f13417fa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 327ms/step - accuracy: 0.5352 - loss: 0.7399 - val_accuracy: 0.8500 - val_loss: 0.5683 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 268ms/step - accuracy: 0.7087 - loss: 0.6029 - val_accuracy: 0.8733 - val_loss: 0.5590 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 264ms/step - accuracy: 0.7980 - loss: 0.5207 - val_accuracy: 0.9100 - val_loss: 0.4794 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 271ms/step - accuracy: 0.8440 - loss: 0.4551 - val_accuracy: 0.8567 - val_loss: 0.4769 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 278ms/step - accuracy: 0.8851 - loss: 0.3828 - val_accuracy: 0.9233 - val_loss: 0.3945 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 267ms/step - accuracy: 0.8901 - loss: 0.3457 - val_accuracy: 0.9533 - val_loss: 0.2852 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 262ms/step - accuracy: 0.9121 - loss: 0.3061 - val_accuracy: 0.9500 - val_loss: 0.2639 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 269ms/step - accuracy: 0.9144 - loss: 0.2858 - val_accuracy: 0.9467 - val_loss: 0.2494 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 270ms/step - accuracy: 0.9090 - loss: 0.2700 - val_accuracy: 0.9533 - val_loss: 0.2280 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 262ms/step - accuracy: 0.9295 - loss: 0.2359 - val_accuracy: 0.9433 - val_loss: 0.2207 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 273ms/step - accuracy: 0.9337 - loss: 0.2247 - val_accuracy: 0.9667 - val_loss: 0.1728 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 271ms/step - accuracy: 0.9298 - loss: 0.2082 - val_accuracy: 0.9667 - val_loss: 0.1783 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 269ms/step - accuracy: 0.9372 - loss: 0.2138 - val_accuracy: 0.9733 - val_loss: 0.1492 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 269ms/step - accuracy: 0.9341 - loss: 0.2003 - val_accuracy: 0.9700 - val_loss: 0.1561 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 266ms/step - accuracy: 0.9413 - loss: 0.1970 - val_accuracy: 0.9733 - val_loss: 0.1315 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset_echo_sopk/test',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qKzHTBkEdtt",
        "outputId": "3032c868-de4d-4eb4-fdcc-b6c481784422"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 245ms/step - accuracy: 0.9754 - loss: 0.1292\n",
            "Test Accuracy: 97.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(\"model_echo_sopk.keras\")\n",
        "# model.save(\"model_echo_sopk.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFljg-GBrS8m",
        "outputId": "03d917a2-4944-41d6-d449-489c4c35d14c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(\"model_echographie_sopk.keras\")"
      ],
      "metadata": {
        "id": "t8bBastYYPNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Chemin de l'image à tester\n",
        "img_path = \"/content/drive/MyDrive/dataset_echo_sopk/test_final/notinfected1.jpg\"\n",
        "\n",
        "# Charger et préparer l'image\n",
        "image = cv2.imread(img_path)                         # Lire l'image\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       # Convertir BGR -> RGB\n",
        "image = cv2.resize(image, (224, 224))                # Redimensionner à la taille du modèle\n",
        "image = image / 255.0                                # Normaliser les pixels entre 0 et 1\n",
        "image = np.expand_dims(image, axis=0)                # Ajouter la dimension batch (1, 224, 224, 3)\n",
        "\n",
        "# Prédiction\n",
        "prediction = model.predict(image)[0][0]\n",
        "\n",
        "# Résultat\n",
        "if prediction < 0.5:\n",
        "    print(f\"L'ovaire est infecté (SOPK détecté) avec {(1 - prediction) * 100:.2f}% de certitude\")\n",
        "else:\n",
        "    print(f\"L'ovaire est sain avec {prediction * 100:.2f}% de certitude\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLxTxsz0EnKH",
        "outputId": "2c6551af-816b-470d-f509-a911909e4f7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "L'ovaire est sain avec 93.52% de certitude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EfficientNetB0**"
      ],
      "metadata": {
        "id": "vuODAdUAEhpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger EfficientNetB0 + ajouter des couches finales\n",
        "# Charger EfficientNetB0 sans la tête finale\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Geler les couches du backbone\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Ajouter nos propres couches\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Binaire\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n"
      ],
      "metadata": {
        "id": "PG0RN3XmGNbg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler et entraîner\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    # callbacks=[early_stop]\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPCrPV8GUjp",
        "outputId": "85fe2af5-ac4f-4f95-e19f-7fb11cbc6057"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - accuracy: 0.5212 - loss: 0.6969 - val_accuracy: 0.6667 - val_loss: 0.6480 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 254ms/step - accuracy: 0.5127 - loss: 0.6954 - val_accuracy: 0.6667 - val_loss: 0.6517 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 261ms/step - accuracy: 0.5180 - loss: 0.6924 - val_accuracy: 0.6667 - val_loss: 0.6571 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 272ms/step - accuracy: 0.5028 - loss: 0.6977 - val_accuracy: 0.6667 - val_loss: 0.6654 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset_echo_sopk/test',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0akN8tJKNNx6",
        "outputId": "001c6f81-3712-46db-826a-26ef03c2f4e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 294ms/step - accuracy: 0.6786 - loss: 0.6437\n",
            "Test Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Chemin de l'image à tester\n",
        "img_path = \"/content/drive/MyDrive/dataset_echo_sopk/test_final/notinfected1.jpg\"\n",
        "\n",
        "# Charger et préparer l'image\n",
        "image = cv2.imread(img_path)                         # Lire l'image\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       # Convertir BGR -> RGB\n",
        "image = cv2.resize(image, (224, 224))                # Redimensionner à la taille du modèle\n",
        "image = image / 255.0                                # Normaliser les pixels entre 0 et 1\n",
        "image = np.expand_dims(image, axis=0)                # Ajouter la dimension batch (1, 224, 224, 3)\n",
        "\n",
        "# Prédiction\n",
        "prediction = model.predict(image)[0][0]\n",
        "\n",
        "# Résultat\n",
        "if prediction < 0.5:\n",
        "    print(f\"L'ovaire est infecté (SOPK détecté) avec {(1 - prediction) * 100:.2f}% de certitude\")\n",
        "else:\n",
        "    print(f\"L'ovaire est sain avec {prediction * 100:.2f}% de certitude\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6tK6eY-Gafz",
        "outputId": "552c45f5-0a54-41f4-d1f5-4349fc2862f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "L'ovaire est sain avec 59.06% de certitude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNetV2"
      ],
      "metadata": {
        "id": "tfK94tH3I8PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger MobileNetV2 + ajouter des couches personnalisées\n",
        "# Charger MobileNetV2 sans la tête (classification)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Geler les couches du modèle pré-entraîné\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Ajouter les couches de classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuqd9NfdJWAc",
        "outputId": "152b31a1-dcbf-4a24-f154-0b36f7dac97c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compiler et entraîner\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    # callbacks=[early_stop]\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv0lLeokJrcX",
        "outputId": "0fd37f22-49ac-485f-f416-18bab28f9ef6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 293ms/step - accuracy: 0.8449 - loss: 0.3438 - val_accuracy: 1.0000 - val_loss: 0.0255 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 248ms/step - accuracy: 0.9989 - loss: 0.0229 - val_accuracy: 1.0000 - val_loss: 0.0078 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 255ms/step - accuracy: 0.9996 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 0.0036 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 249ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 255ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0015 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 253ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0011 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 268ms/step - accuracy: 0.9999 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 7.0769e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 258ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 5.5878e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 263ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 4.4472e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 9.1895e-04 - val_accuracy: 1.0000 - val_loss: 4.9002e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 267ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 3.3684e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 259ms/step - accuracy: 1.0000 - loss: 9.3275e-04 - val_accuracy: 1.0000 - val_loss: 3.6673e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 260ms/step - accuracy: 1.0000 - loss: 4.5779e-04 - val_accuracy: 1.0000 - val_loss: 2.8113e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 3.5467e-04 - val_accuracy: 1.0000 - val_loss: 2.3790e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 250ms/step - accuracy: 1.0000 - loss: 4.1667e-04 - val_accuracy: 1.0000 - val_loss: 1.7214e-04 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer le modèle\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset_echo_sopk/test',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW9EH1o7NRV1",
        "outputId": "36032184-cd1e-4acc-be25-4bcd2e72dee7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.9647 - loss: 0.1437\n",
            "Test Accuracy: 96.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Chemin de l'image à tester\n",
        "img_path = \"/content/drive/MyDrive/dataset_echo_sopk/test_final/notinfected1.jpg\"\n",
        "\n",
        "# Charger et préparer l'image\n",
        "image = cv2.imread(img_path)                         # Lire l'image\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       # Convertir BGR -> RGB\n",
        "image = cv2.resize(image, (224, 224))                # Redimensionner à la taille du modèle\n",
        "image = image / 255.0                                # Normaliser les pixels entre 0 et 1\n",
        "image = np.expand_dims(image, axis=0)                # Ajouter la dimension batch (1, 224, 224, 3)\n",
        "\n",
        "# Prédiction\n",
        "prediction = model.predict(image)[0][0]\n",
        "\n",
        "# Résultat\n",
        "if prediction < 0.5:\n",
        "    print(f\"L'ovaire est infecté (SOPK détecté) avec {(1 - prediction) * 100:.2f}% de certitude\")\n",
        "else:\n",
        "    print(f\"L'ovaire est sain avec {prediction * 100:.2f}% de certitude\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW-8VHGfJzqd",
        "outputId": "f548134e-7b7a-459d-d0cb-a4fbce12876a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ce9237f1da0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "L'ovaire est sain avec 94.35% de certitude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SOPWKelb_-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}